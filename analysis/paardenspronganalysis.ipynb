{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paardensprong\n",
    "The goal of this notebook is to do the exploratory analysis needed to build a model that can predict the probability of a correct and timely guess. \n",
    "I want to use that model to select puzzles which I have a ~50% chance of solving\n",
    "\n",
    "* Theoretical model:\n",
    "  * Basic puzzle:\n",
    "    * Finding direction in the word\n",
    "    * Finding starting point in the word\n",
    "\n",
    "  * Recognizing the word\n",
    "    * Word frequency in normal language\n",
    "    * Having seen it recently (in a puzzle) - not implemented yet\n",
    "    * Pronunciation matches writing it down (e.g. fauteuil is very hard) - not implemented yet\n",
    "\n",
    "  * Puzzle (inadvertent biases from my stsarting point, so that I take too long to switch to the correct point)\n",
    "    * Direction \n",
    "    * Starting point\n",
    "    * And sometimes I lose track when a single letter occurs very frequently - not implemented yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import importlib.resources\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PLAYERNAME = os.getenv('playername')\n",
    "\n",
    "database_url_prod = os.getenv('PROD_DATABASE_URL').replace('postgresql', 'postgresql+psycopg')\n",
    "engine_prod = create_engine(database_url_prod)\n",
    "\n",
    "database_url_dev = os.getenv('DATABASE_URL').replace('postgresql', 'postgresql+psycopg')\n",
    "engine_dev = create_engine(database_url_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine_prod.connect() as conn:\n",
    "    games = pd.read_sql_query('SELECT * FROM paardensprong.games', con=conn, index_col='game_id')\n",
    "    guesses = pd.read_sql_query('SELECT * FROM paardensprong.guesses', con=conn, index_col='guess_id')\n",
    "\n",
    "guesses_relevant = (guesses.set_index('game_id')\n",
    "                    .rename(columns={'correct': 'GuessCorrect'})\n",
    "                    [['guess_time', 'GuessCorrect']]\n",
    "                    )                           \n",
    "\n",
    "df = (games\n",
    "      # Drop games which have no guess - probably time out because of long loading times\n",
    "      .join(guesses_relevant, how='inner')\n",
    "      .query('playername == @PLAYERNAME')\n",
    "      .assign(PuzzleTimeSec = lambda df: (df['guess_time'] - df['start_time']).dt.seconds,\n",
    "              # The on time is a bit strict; since you need a few seconds typing time\n",
    "              # But that's on purpose: it makes sense to train to have a bit of spare time\n",
    "              # And it helps the model since you have just a few more unsuccessfulls to train on\n",
    "              OnTime = lambda df: df['PuzzleTimeSec'].lt(30),\n",
    "              Success = lambda df: df['GuessCorrect'] & df['OnTime'],\n",
    "              )\n",
    "      # A few answers were given extremely late; probably when reconnecting\n",
    "      .query('PuzzleTimeSec < 120')\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we immediately see the problem: ~90% is solved successfully on time; so I want the most challenging puzzles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Success'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PuzzleTimeSec'].hist(bins=range(0, df['PuzzleTimeSec'].max() + 5, 5))\n",
    "df['PuzzleTimeSec'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = importlib.resources.files('tweevoortwaalf.Data').joinpath('suitable_8_letter_words.txt')\n",
    "eightletterwords = pd.read_csv(DATA_PATH, header=None).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While solving the puzzle, I often look for illogical consecutive letters. Then we know that can't be correct, so the solution should go the other way around. First, we generalize this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "vectorizer.fit(eightletterwords)\n",
    "ngrams_occurences_total = vectorizer.transform(eightletterwords).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easyness_score(woord, vectorizer=vectorizer):\n",
    "    \"Sums all transitions of letters -> the higher, the more logical\"\n",
    "    ngrams_occurences_word = vectorizer.transform([woord])\n",
    "    return (ngrams_occurences_word * ngrams_occurences_total).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction\n",
    "The theory is as follows: \n",
    "To find the logical direction, you check the second least likely transition per direction\n",
    "The least likely transition of two characters would be the word boundary\n",
    "If there is another one, then perhaps the word is not written in that direction\n",
    "Then, we compare the second least likely transition per direct\n",
    "We add a compensation because there can be 2 impossible transitions in the wrong direction\n",
    "\n",
    "This could probably be improved by explicitly checking the least likely transition is a likely word boundary (e.g. zwerfkei: wz is illogical transition AND ew is impossible end of word, even though \"ziek\"  is a good start of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_single_direction(word):\n",
    "    \"\"\"Check the second least likely transition, including the transition across the word boundary\"\"\"\n",
    "    circular_word = word + word[0]\n",
    "    logical = []\n",
    "    for i in range(len(circular_word) - 1):\n",
    "        logical.append(easyness_score(circular_word[i] + circular_word[i + 1]))\n",
    "    return sorted(logical)[1]\n",
    "\n",
    "\n",
    "def logical_correct_direction(word, compensation=0.5):\n",
    "    \"\"\"Compare both directions\"\"\"\n",
    "    logical_actual_direction = logical_single_direction(word)\n",
    "    logical_wrong_direction = logical_single_direction(word[::-1])\n",
    "    return (logical_actual_direction + compensation) / (logical_wrong_direction + compensation)\n",
    "\n",
    "direction = df['answer'].apply(logical_correct_direction)\n",
    "directionbins = pd.qcut(direction, q=5)\n",
    "df.groupby(directionbins)['Success'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we see that words where the direction is clear are more often guessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[direction.nlargest(5).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word boundary\n",
    "Assuming the correct direction, how special is the actual transition of the word boundary compared to the other character transitions?\n",
    "This is not perfect yet.. e.g. ox and xi are special in oxidator, but that doesnt make the X the logical starting letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_word_boundary(word, compensation=0.5):\n",
    "    circular_word = word + word[0]\n",
    "    logical = []\n",
    "    for i in range(len(circular_word) - 1):\n",
    "        logical.append(1 / (easyness_score(circular_word[i] + circular_word[i + 1]) + compensation))\n",
    "    return logical[-1] / sum(logical)\n",
    "\n",
    "wordboundary = df['answer'].apply(logical_word_boundary)\n",
    "wordboundarybins = pd.qcut(wordboundary, q=5)\n",
    "df.groupby(wordboundarybins)['Success'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordboundaryscore = eightletterwords.apply(logical_word_boundary)\n",
    "eightletterwords.loc[wordboundaryscore.nsmallest(10).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing word\n",
    "I hypothesize that recognizing the word is easier if you have seen the word recently. That would be related to how often you see it in normal use of the language, and whether it was played recently (which is not implemented yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = pd.read_csv('../tweevoortwaalf/Data/wordlist.csv')\n",
    "# There are some duplicates in Word for words including ij, where one occurs very infrequently\n",
    "frequency = wordlist.query('Length == 8').groupby('Word')['Frequency'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df.merge(frequency, left_on='answer', right_index=True)\n",
    "frequencybins = pd.qcut(new['Frequency'], q=5)\n",
    "df.groupby(frequencybins)['Success'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the least frequent words are guessed less often - this is in line with the hypothesis that especially the words that you don't know are much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle: startpoint\n",
    "I'm not convinced the puzzle characteristics will have a strong effect; I think the word is more important. But it would be silly to rule out my own biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('startpoint')['Success'].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data, it's definitely impossible to rule out the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle: direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('direction')['Success'].value_counts(normalize=True).unstack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweevoortwaalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
