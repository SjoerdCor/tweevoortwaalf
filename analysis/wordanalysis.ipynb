{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twaalfletterwoorden = (\n",
    "    pd.read_csv(\"../tweevoortwaalf/Data/suitable_12_letter_words.txt\", header=None)\n",
    "    .squeeze()\n",
    "    .rename(\"Word\")\n",
    ")\n",
    "achtletterwoorden = (\n",
    "    pd.read_csv(\"../tweevoortwaalf/Data/suitable_8_letter_words.txt\", header=None)\n",
    "    .squeeze()\n",
    "    .rename(\"Word\")\n",
    ")\n",
    "negenletterwoorden = (\n",
    "    pd.read_csv(\"../tweevoortwaalf/Data/suitable_9_letter_words.txt\", header=None)\n",
    "    .squeeze()\n",
    "    .rename(\"Word\")\n",
    ")\n",
    "\n",
    "print(f\"{len(achtletterwoorden)} achtletterwoorden\")\n",
    "print(f\"{len(negenletterwoorden)} negenletterwoorden\")\n",
    "print(f\"{len(twaalfletterwoorden)} twaalfletterwoorden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completeness of the word lists\n",
    "A lot of effor was put into creating word lists of as high quality as possible. We want to know both recall and precision of these word lists: do they capture all words that are actually used in the game show, and could this all be used. \n",
    "\n",
    "If both of these are ok, we can actually use the lists for training and analyzing\n",
    "\n",
    "If the precision is too low, meaning there are many words on the word lists that wouldn't actually occur in Twee Voor Twaalf, practice would be inefficient: we might be puzzling for inexistent or very rare words - and even training for patters that do not exist. It can be demotivating too, if the puzzle is undoable because you have to guess an inexistent word. There is no easy way to automatically check this, since we cannot look into the heads of the editors of the show. However, based on actually playing this game, this does not seem to be a big problem\n",
    "If the recall is too low, meaning there are many words used in the show that are not on the word lists, we may become overconfident, learning not enough patters, and or memorizing words where other options are available. If the recall is very high, one could even suffice with memorizing the entire word list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"date\": \"2024-09-13\",\n",
    "        \"round\": 1,\n",
    "        \"eightletterword\": \"executie\",\n",
    "        \"lookedup\": 73,\n",
    "        \"n_incorrect\": 2,\n",
    "        \"n_missing\": 1,\n",
    "        \"n_uithoofd\": 6,\n",
    "        \"n_letters_bought\": 5,\n",
    "        \"twelveletterword\": \"afwasborstel\",\n",
    "        \"word_correct\": True,\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2024-09-13\",\n",
    "        \"round\": 2,\n",
    "        \"eightletterword\": \"epidemie\",\n",
    "        \"lookedup\": 45,\n",
    "        \"n_incorrect\": 1,\n",
    "        \"n_missing\": 0,\n",
    "        \"n_uithoofd\": 7,\n",
    "        \"n_letters_bought\": 1,\n",
    "        \"twelveletterword\": \"tuchtcollege\",\n",
    "        \"word_correct\": True,\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2024-09-20\",\n",
    "        \"round\": 1,\n",
    "        \"nineletterword\": \"slotdebat\",\n",
    "        \"lookedup\": 145,  # To check\n",
    "        \"n_incorrect\": 4,  # to check\n",
    "        \"n_missing\": 4,  # To check\n",
    "        \"n_uithoofd\": 2,  # To check\n",
    "        \"n_letters_bought\": 8,  # To check\n",
    "        \"twelveletterword\": \"beschermlaag\",\n",
    "        \"word_correct\": False,\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2024-09-20\",\n",
    "        \"round\": 3,\n",
    "        \"nineletterword\": \"weigering\",\n",
    "        \"lookedup\": 30,  # To check\n",
    "        \"n_incorrect\": 0,\n",
    "        \"n_missing\": 0,\n",
    "        \"n_uithoofd\": 7,\n",
    "        \"n_letters_bought\": 3,\n",
    "        \"twelveletterword\": \"achtervolger\",\n",
    "        \"word_correct\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_score(row):\n",
    "    if not row[\"word_correct\"]:\n",
    "        return 0\n",
    "    base = 500\n",
    "    n_wrong = row[\"n_incorrect\"] + row[\"n_missing\"]\n",
    "    bonus = {0: 100, 1: 75, 2: 50, 3: 25}\n",
    "    bonus_received = bonus.get(n_wrong, 0)\n",
    "\n",
    "    return (\n",
    "        base\n",
    "        - row[\"lookedup\"]\n",
    "        - 10 * row[\"n_letters_bought\"]\n",
    "        + bonus_received\n",
    "        + 10 * row[\"n_uithoofd\"]\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data).assign(\n",
    "    score=lambda df: df.apply(calculate_score, axis=\"columns\")\n",
    ")\n",
    "\n",
    "true_12_letter_words = [\n",
    "    \"koekoeksklok\",\n",
    "    \"kalenderjaar\",\n",
    "    \"zeekomkommer\",\n",
    "    \"zonnestelsel\",\n",
    "    \"biljartlaken\",\n",
    "    \"kroonprinses\",\n",
    "    \"ballingschap\",\n",
    "    # season september 2024\n",
    "    \"krachtmeting\",\n",
    "] + df[\"twelveletterword\"].tolist()\n",
    "\n",
    "true_8_letter_words = [\"fauteuil\"] + df[\"eightletterword\"].dropna().tolist()\n",
    "true_9_letter_words = [\"stopwatch\", \"camembert\"] + df[\n",
    "    \"nineletterword\"\n",
    "].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordlist = pd.read_csv(\"../tweevoortwaalf/Data/wordlist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suitable_words = pd.concat([achtletterwoorden, negenletterwoorden, twaalfletterwoorden])\n",
    "suitability_cols = [\n",
    "    \"AllLowercase\",\n",
    "    \"AllBasicAlpha\",\n",
    "    \"ZelfstandigNaamwoord\",\n",
    "    \"IsEnkelvoud\",\n",
    "]\n",
    "\n",
    "all_words = pd.DataFrame(\n",
    "    {\n",
    "        \"word\": true_8_letter_words + true_9_letter_words + true_12_letter_words,\n",
    "        \"Length\": [8] * len(true_8_letter_words)\n",
    "        + [9] * len(true_9_letter_words)\n",
    "        + [12] * len(true_12_letter_words),\n",
    "    }\n",
    ")\n",
    "true_words = (\n",
    "    all_words.merge(suitable_words, how=\"left\", left_on=\"word\", right_on=\"Word\")\n",
    "    .assign(InMyWordlists=lambda df: df[\"Word\"].notnull())\n",
    "    .drop(columns=[\"Word\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_binomial(p, n):\n",
    "    return ((p * (1 - p)) / n) ** 0.5\n",
    "\n",
    "\n",
    "def bootstrap_sample(sample, alpha=0.95, n_bootstrap=10_000):\n",
    "    replications = np.array(\n",
    "        [\n",
    "            np.random.choice(sample, len(sample), replace=True)\n",
    "            for _ in range(n_bootstrap)\n",
    "        ]\n",
    "    )\n",
    "    recalls = np.mean(replications, axis=1)\n",
    "    p = ((1.0 - alpha) / 2.0) * 100\n",
    "\n",
    "    lower = np.percentile(recalls, p)\n",
    "    median = np.percentile(recalls, 50)\n",
    "    upper = np.percentile(recalls, 100 - p)\n",
    "\n",
    "    return lower, median, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = true_words.groupby(\"Length\").agg(\n",
    "    Recall=pd.NamedAgg(\"InMyWordlists\", \"mean\"),\n",
    "    N=pd.NamedAgg(\"word\", \"count\"),\n",
    "    Bootstrap=pd.NamedAgg(\"InMyWordlists\", bootstrap_sample),\n",
    ")\n",
    "\n",
    "results[[\"BootstrapMin\", \"BootstrapMedian\", \"BootstrapMax\"]] = pd.DataFrame(\n",
    "    results[\"Bootstrap\"].tolist(), index=results.index\n",
    ")\n",
    "results = results.drop(columns=[\"Bootstrap\"])\n",
    "results.assign(\n",
    "    SE=lambda df: df.apply(\n",
    "        lambda row: standard_error_binomial(row[\"Recall\"], row[\"N\"]), axis=\"columns\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where in the funnel are words lost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_info = true_words.merge(\n",
    "    df_wordlist[[\"WordOriginal\", \"Word\", \"Length\", \"InVanDale\"] + suitability_cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"word\",\n",
    "    right_on=\"WordOriginal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_info.agg(\n",
    "    Total=pd.NamedAgg(\"word\", \"count\"),\n",
    "    InWordlist=pd.NamedAgg(\"WordOriginal\", \"count\"),\n",
    "    InVanDale=pd.NamedAgg(\"InVanDale\", \"sum\"),\n",
    "    Suitable=pd.NamedAgg(\"InMyWordlists\", \"sum\"),\n",
    ").fillna(method=\"backfill\", axis=\"columns\")[\"word\"].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of words\n",
    "By analyzing the common structure of 12-letter words, we can improve the guessing and buying process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occurrence and placement of letters\n",
    "We can improve our strategy of buying letters by knowing which letters occur at which places. The first letter often helps most. Some letters we may not have to buy, because we can figure out where they will go. We will calculate for each letter how often it occurs on each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurence_ngrams(ngram_length=1, wordlist=twaalfletterwoorden):\n",
    "    cv = CountVectorizer(analyzer=\"char_wb\", ngram_range=(ngram_length, ngram_length))\n",
    "    occurences = cv.fit_transform(twaalfletterwoorden)\n",
    "    df = pd.DataFrame(occurences.toarray(), columns=cv.get_feature_names_out()).rename(\n",
    "        columns=lambda s: s.replace(\" \", \"_\")\n",
    "    )\n",
    "    return df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_occurences = get_occurence_ngrams()\n",
    "letter_occurences.sort_values(ascending=False).to_frame(\"# occurrences in word list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_position = (\n",
    "    twaalfletterwoorden.apply(lambda x: list(enumerate(x, start=1)))\n",
    "    .explode()\n",
    "    .to_frame()\n",
    "    .assign(\n",
    "        Location=lambda df: df[\"Word\"].apply(lambda tup: tup[0]),\n",
    "        Letter=lambda df: df[\"Word\"].apply(lambda tup: tup[1]),\n",
    "    )\n",
    ")\n",
    "letter_position_frequency = (\n",
    "    letter_position.groupby(\"Letter\")[\"Location\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "(\n",
    "    letter_position_frequency.agg([\"idxmax\", \"max\"], axis=\"columns\")\n",
    "    .sort_values(\"max\", ascending=False)\n",
    "    .rename(columns={\"idxmax\": \"MostCommonPosition\", \"max\": \"Percentage\"})\n",
    "    .astype({\"MostCommonPosition\": int})\n",
    "    .style.format({\"Percentage\": \"{:.1%}\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a table of 27 letters * 12 positions is impossible to learn, we try to cluster which letters are of the same type. We use TSNE to visualise this and find there are 5 distinct groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=5, random_state=42)\n",
    "transformed = tsne.fit_transform(letter_position_frequency)\n",
    "transformed = pd.DataFrame(\n",
    "    transformed,\n",
    "    index=letter_position_frequency.index,\n",
    "    columns=[\"FirstComponent\", \"SecondComponent\"],\n",
    ")\n",
    "ax = transformed.plot(kind=\"scatter\", x=\"FirstComponent\", y=\"SecondComponent\")\n",
    "for k, v in transformed.iterrows():\n",
    "    ax.annotate(\n",
    "        k,\n",
    "        v,\n",
    "        xytext=(10, -5),\n",
    "        textcoords=\"offset points\",\n",
    "        family=\"sans-serif\",\n",
    "        fontsize=18,\n",
    "        color=\"darkslategrey\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_groups = [\n",
    "    list(\"bzvw\"),\n",
    "    list(\"mpskh\"),\n",
    "    list(\"fdtrlg\"),\n",
    "    list(\"uaoiyc\"),\n",
    "    list(\"jĳne\"),\n",
    "]\n",
    "for group in letter_groups:\n",
    "    display(letter_position_frequency.loc[group].style.format(\"{:.1%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of letter placements, we have a small number of groups:\n",
    "1. b,z,v, w: Very often the starting letter. Basically never the last letter(s)\n",
    "2. m,p,s,k (h): are often at the beginning or, a bit more rarely at the end\n",
    "3. f,d,t,r,l,g: are often the last letter. G stands out as very often the last letter, or at the beginning. The rest is seldom at the beginning\n",
    "4. u,a,o,i,y,c: Often the second or third letter \n",
    "5. e,n,j,ij: Often the second-to-last letter (for \"n\", this is often \"ing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occurrence of letter combinations\n",
    "We can improve even more, and buy much fewer letters, by knowing which letters come together, so we have to buy only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twograms = get_occurence_ngrams(2)\n",
    "twograms.index = pd.MultiIndex.from_arrays(\n",
    "    [twograms.index.str[0], twograms.index.str[1]],\n",
    "    names=[\"FirstLetter\", \"SecondLetter\"],\n",
    ")\n",
    "twograms.nlargest(15).to_frame(\"N Occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: \n",
    "- \"je\", \"ch\", \"er\", \"rij\", \"ng\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_secondletter_given_first_letter = twograms.to_frame(\"Occurrences\").assign(\n",
    "    Percentage=lambda df: df[\"Occurrences\"]\n",
    "    / df.groupby(\"FirstLetter\")[\"Occurrences\"].sum()\n",
    ")\n",
    "display(odds_secondletter_given_first_letter.nlargest(15, \"Percentage\"))\n",
    "odds_secondletter_given_first_letter.plot(\n",
    "    kind=\"scatter\", x=\"Occurrences\", y=\"Percentage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_firstletter_given_second_letter = twograms.to_frame(\"Occurrences\").assign(\n",
    "    Percentage=lambda df: df[\"Occurrences\"]\n",
    "    / df.groupby(\"SecondLetter\")[\"Occurrences\"].sum()\n",
    ")\n",
    "display(odds_firstletter_given_second_letter.nlargest(15, \"Percentage\"))\n",
    "odds_firstletter_given_second_letter.plot(\n",
    "    kind=\"scatter\", x=\"Occurrences\", y=\"Percentage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer letter combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words often end on \"ing\" or \"(a)tie\". Dont forget \"teit\" and \"schap\", ending \"ter\" and \"der\" and \"meester\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threegrams = get_occurence_ngrams(3)\n",
    "threegrams.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgrams = get_occurence_ngrams(4)\n",
    "fourgrams.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegrams = get_occurence_ngrams(5)\n",
    "fivegrams.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixgrams = get_occurence_ngrams(6)\n",
    "sixgrams.nlargest(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special words\n",
    "We look for words that are immediately recognizable because they have one letter very often, or a specific combination of two letters that is unique, so we don't have to buy any letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One letter occurs many times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=\"char_wb\", ngram_range=(1, 1))\n",
    "occurences = cv.fit_transform(twaalfletterwoorden)\n",
    "letter_occs = pd.DataFrame(\n",
    "    occurences.toarray(), columns=cv.get_feature_names_out()\n",
    ").rename(columns=lambda s: s.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_most_occuring_words(letter, letter_occs=letter_occs):\n",
    "    return letter_occs.loc[lambda df: df[letter] == df[letter].max(), letter]\n",
    "\n",
    "\n",
    "def print_special_words(letter):\n",
    "    most_occuring_words_index = return_all_most_occuring_words(letter)\n",
    "    if len(most_occuring_words_index) <= 3:\n",
    "        most_occuring_words = twaalfletterwoorden.loc[\n",
    "            most_occuring_words_index.index\n",
    "        ].tolist()\n",
    "        print(\n",
    "            f\"The letter {letter!r} occurs at most {most_occuring_words_index.max()} times\"\n",
    "        )\n",
    "        print(\", \".join(most_occuring_words))\n",
    "        print()\n",
    "\n",
    "\n",
    "for letter in letter_occs.columns:\n",
    "    print_special_words(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = letter_occs.loc[:, lambda df: ~df.columns.str.contains(\"_\")].columns\n",
    "\n",
    "for first_letter, second_letter in itertools.combinations(all_letters, 2):\n",
    "    words_with_combination_index = letter_occs.loc[\n",
    "        lambda df: df[first_letter].gt(0) & df[second_letter].gt(0)\n",
    "    ].index\n",
    "    if 0 < len(words_with_combination_index) <= 3:\n",
    "        most_occuring_words = twaalfletterwoorden.loc[\n",
    "            words_with_combination_index\n",
    "        ].tolist()\n",
    "        print(f\"The combination of {first_letter!r} and {second_letter!r}  is rare\")\n",
    "        print(\", \".join(most_occuring_words))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweevoortwaalf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
